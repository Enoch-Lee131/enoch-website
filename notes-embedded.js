// Embedded notes fallback - works without a local server
// This file is auto-generated from /notes/*.md files
// Run 'node sync-notes.js' to regenerate after editing notes
// Last updated: 2025-11-17

const EMBEDDED_NOTES = {
    'Voyager': {
        metadata: {
            title: 'VOYAGER: An Open-Ended Embodied Agent with Large Language Models',
            date: '2025-11-11',
            tags: ["LLM","AI Research"],
            summary: ''
        },
        content: `## Motivation and Problem Statement
The motivation stems from the limitations of current AI in complex, interactive, and open-ended environments.  

â€¢ Gap between Declarative and Proceduarl Knowledge: LLMs excel at reasoning (declarative knowledge) but often struggle with interactive, physical tasks (procedural knowledge). 
â€¢ Limitations of Traditional RL: Traditional RL agents are often black boxes, require vast amount of data, and struggle with catastrophic forgetting and generalization in lifelong learning scenarios.  
â€¢ Need for Open-Ended Explorations: Existing LLM-based agents often struggle with open-ended exploration, progressive skill acquisition, and generalizing learned behaviors to novel situations.  

**Core Idea of Voyager** 
The paper VOYAGER introduces a novel LLM-powered agent designed for lifelong learning and open-ended exploration in the Minecraft environment.  

## System Architecture 
Voyager consists of three novel components that work together to enable lifelong learning:
â€¢ Automatic Cirriculum: Generates a sequence of progressively challenging tasks for the agent. It is dynamically generated by GPT-4, which considers the agent's current state (inventory, exploration progress, completed skills) to propose tasks that maximize novelty and exploration.   
â€¢ Skill Library: Stores and manages executable code programs (skills) that represent complex, temporally extended behaviors (e.g., craftStonePickaxe()). Skills are stored as code, indexed by their natural language description embedding, making them interpretable, reusable, and compositional. Retrieval based on task plans allows for rapid compounding of abilities and mitigates catastrophic forgetting.  
â€¢ Iterative Prompting: Generates and refines executable code for a task using environment feedback. It involves (1) Code Generation by GPT-4 (2) Execution in the Minecraft environment (3) Feedback Loop (incorporating environment observations and execution errors/traces) to refine the code. (4) Self-Verification to confirm task completion before the new skill is added to the Skill Library.   

![Voyager](images/voyager1.png) 

**Analysis of Key Components** 
â€¢ Lifelong Learning via Library: The Skill Library is the engine for lifelong learning. It allows the agent to build on its previous successes, and ensure new, complex skills are combinations of simpler, verified skills. 
â€¢ Effectiveness of Components: Ablation studies show that all three components are critical for Voyager's superior performance in exploration and milestone unlocking. 

**Insight** 
The key insight is that LLMs can serve as effective universal controllers for embodied agents by generating, debuggin, and composing high-level interpretable programs (skills). The combination of an automatic, novelty-seeking cirriculum within the skill library and iterative self-correction mechanism results in an agent capable of in-context lifelong learning. 

## References
â€¢ VOYAGER: An Open-Ended Embodied Agent with Large Language Models. arXiv: [https://arxiv.org/pdf/2305.16291]

`
    },
    'CER': {
        metadata: {
            title: 'Contextual Experience Replay for Self-Improvement of Language Agents',
            date: '2025-10-20',
            tags: ["LLM","AI Research"],
            summary: ''
        },
        content: `## Motivation and Problem Statement
Language agents (e.g., GPT-based web navigators) can perform tasks like shopping, posting, or searching across websites. However, even state-of-the-art systems achieve only ~20% success on benchmarks like WebArena and VisualWebArena, while humans reach 78â€“88%.
The main issue: Agents lack prior, environment-specific knowledge, forcing them to re-explore from scratch each time.  

It is resource-intesive to fine-tune an agent for each environment, and there's no mechanism for continual learning at inference time. The agent forgets what it learned after each episode. Thus, the authors propose Contextual Experience Replay (CER) - a way for an agent to learn continuously from its own trajectories, without retraining.  

**Core Idea of CER** 

Inspired by experience replay in reinforcement learning, CER introduces an in-context replay mechanism for LLM agents.  

The core cycle:  
â€¢ Distill knowledge from past trajectories into â€œexperiences.â€  
â€¢ Store those experiences in a dynamic memory buffer.  
â€¢ Retrieve relevant experiences for a new task.  
â€¢ Replay (inject) them into the LLMâ€™s context before generating actions.  
â€¢ This allows self-improvement at inference time, purely through prompt-based learning (no parameter updates).  

## System Architecture

**Distillation Module**   
â€¢ Input: Full trajectories (stateâ€“actionâ€“observation logs).  
â€¢ Output: Two experience types:  
â€¢ Environment Dynamics: Summaries of how the website behaves (e.g., â€œClicking â€˜Nextâ€™ loads product detailsâ€).  
â€¢ Skills: Abstract, reusable procedural patterns (e.g., â€œNavigate to {forum name} â†’ Click â€˜New Postâ€™ â†’ Submitâ€).  
â€¢ The distiller uses an LLM to abstract and generalize each trajectory, writing results in natural language form.  
â€¢ It also references the existing buffer to avoid redundancy and ensure continual accumulation.  

**Experience Buffer**  
â€¢ A memory repository that accumulates distilled knowledge.  
â€¢ Each entry contains metadata (domain, context, task type) for retrieval indexing.  

**Retriever Module**  
â€¢ Uses semantic similarity search to select top-k relevant experiences for a given new task.  
â€¢ Retrieval is separated for skills and dynamics.  
â€¢ The selected experiences are converted to natural language summaries, which are then integrated into the modelâ€™s prompt.  

**Decision-Making with Contextual Replay**  
â€¢ Retrieved experiences are mapped to a natural language form.  
â€¢ These are merged into the current context ð¶ to create an augmented prompt.  
â€¢ The LLM now reasons with prior context and produces more accurate, efficient actions.  

**Online, Offline, and Hybrid Learning Modes**  
â€¢ Online: Agent starts with zero experience; learns after each task. Self-generated during inference.  
â€¢ Offline: Experiences distilled from pre-collected trajectories; no new learning during inference.	Human-annotated or LLM-generated logs.  
â€¢ Hybrid": Starts with offline experience, continues to learn online. Both offline and self-generated.  

![CER](images/CER.png) 

Even when trained with failed trajectories, CER still improved baseline performance â€” meaning the LLM distillation step is capable of extracting useful partial skills from imperfect data.  

**Analysis of Key Components** 
â€¢ Skill Distillation: Encodes procedural reasoning (steps, actions).Boosts generalization to unseen tasks.  
â€¢ Dynamics Distillation: Encodes environment interaction patterns (state transitions). Speeds up navigation and contextual understanding.  
â€¢ Replay Integration: Contextually inserts past experiences.Improves token efficiency and reasoning depth.  

**Insight** 
LLMs already perform in-context learning â€” they can adapt based on the structure of their prompt. CER leverages this by turning long-term memory into natural language context.
Thus, it transforms episodic experiences into explicit prompts, guiding future reasoning steps.  

It approximates the benefit of experience replay in RL (which reuses old trajectories to stabilize learning) but without gradient updates â€” making it practical for inference-time improvement.  

**Difference between Reflexion** 
Reflexion is designed to make an LLM self-correct within an episode (or a small number of retries). It adds a meta-cognitive feedback loop â€” after the agent completes (or fails) a task, it generates a natural language reflection. So Reflexion is: 1. Local: Focused on iterative self-improvement inside a single task or short-term retry cycle. 2. Episodic: The reflections typically vanish once the environment resets. 3. Reactive: It triggers reflection after failure. 4. Shallow memory: No persistent or structured long-term knowledge.  

CER extends beyond this â€œsingle-task reflectionâ€ into persistent, structured, cross-task learning. It formalizes reflection into two explicit knowledge types (skills, dynamics). It adds a retrieval mechanism so these reflections can be re-used contextually later. It creates a scalable memory substrate that grows and evolves with experience. 

                     +----------------------+
                     |  Reflexion           |
                     |  (Self-Feedback Loop)|
                     +----------------------+
                                â†“
                 (Distill experiences persistently)
                                â†“
                     +----------------------+
                     |  CER                |
                     |  (Cross-task self-  |
                     |   improvement loop) |
                     +----------------------+
                                â†“
                  (Store/retrieve at scale with memory)
                                â†“
                     +----------------------+
                     |  Memory Layer Infra  |
                     |  (Vector DB, RAG)    |
                     +----------------------+

CER represents a bridge between â€œstateless chatâ€ and â€œstateful learning.â€
The next generation of persistent agents (like LangGraph stateful DAG agents, or DSPyâ€™s learned retrieval policies) could integrate CER-style distillation as a new memory layer type, enabling:  

â€¢ Structured knowledge formation â€” not just embeddings.  
â€¢ Task-conditioned retrieval â€” relevant experience replay.  
â€¢ Continual, parameter-free learning â€” inference-time self-improvement.  

## References
â€¢ Contextual Experience Replay for Self-Improvement of Language Agents. arXiv: [https://arxiv.org/pdf/2506.06698]

`
    },
    'reflexion': {
        metadata: {
            title: 'Reflexion: Language Agents with Verbal Reinforcement Learning',
            date: '2025-10-18',
            tags: ["LLM","AI Research"],
            summary: ''
        },
        content: `## Motivation
LLMs are increasing used as autonomous agents (e.g., in games, APIs, or reasoning tasks). However:  
â€¢ Reinforcement Learning methods require expensive fine-tuning and large training data. 
â€¢ In-context learning alone provides weak persistent improvement across trials. 

**Summary** 
Key Idea: Verbal Reinforcement  
The authors introduce Reflexion, a framework that replaces parameter updates with linguistic self-feedback. Agents verbally reflect on previous trials' successes or failures, store their their reflections in episodic memory, and use them to improve decisions in future attempts.

## Core Mechanism
Reflxion consists of three LLM-based modules:  
Actor - Generates actions or text (the policy)  
Evaluator - Judges task success and gives a reward (binary, scalar, or textual)  
Self-reflection - Generates natural language feedback summarizing what went wrong and how to improve  

**Memory**:  
â€¢  Short term memory: Current trajectory (actions and observations)  
â€¢ Long term memory: Accumulated textual reflections from prior trials. The Actor's next trial uses both memories as context.  
 

![reflexion](images/reflexion.png)

**Learning Process**

Algorithm follows an iterative reinforcement loop:  
Actor interacts with environment â†’ trajectory Ï„â‚€.  
Evaluator computes reward râ‚€.  
Self-Reflection generates textual feedback srâ‚€.  
Memory updated: mem â† mem âˆª srâ‚€.  
Actor retries task with mem context until success or max trials.  
Effectively, Reflexion uses language-based â€œpolicy optimizationâ€. 
â€”No weight updates, only context updates.  

**Limitations:**. 
â€¢ Local Minima: Reflexion can converge on suboptimal verbal strategies.  
â€¢ Memory Bound: Limited by LLM context window (only 1â€“3 reflections retained).  
â€¢ Evaluation Dependence: Success depends on reliability of evaluators and test cases.  
â€¢ Task Diversity: Reflexion struggles in high-entropy tasks like WebShop, which require creativity and open exploration.  

**Borader Impact:**   
â€¢ Increases transparency of RL: self-reflections are interpretable traces of reasoning.  
â€¢ Enables diagnosable AI behavior, aiding safety audits.  
â€¢ However, amplifies autonomy risksâ€”if reflection text is misaligned, agent may reinforce undesired behavior.  

**Comparison**   
![reflexion2](images/reflxion2.png)

## References
â€¢ Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv: [https://arxiv.org/pdf/2303.11366]

`
    },
    'stateflow': {
        metadata: {
            title: 'StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows',
            date: '2025-10-05',
            tags: ["LLM","AI Research"],
            summary: ''
        },
        content: `## Motivation

**Summary** 

â€¢ The paper introduces StateFlow, a new paradigm that structures Large Language Model (LLM) task-solving as finite state machines (FSMs). Unlike traditional prompting (e.g., ReAct or Plan-and-Solve), StateFlow decomposes multi-step reasoning tasks into explicit states, transitions, and output functions, improving controllability, interpretability, and efficiency.  

What is a FSM? - It is defined as a mathematical model of computation that represents a system as a finite set of states and transitions between them. Each state corresponds to a particular condition or stage of the process, and transitions occur in response to inputs or events.  

## Problem

Existing frameworks like ReAct (reason + act) or AutoGen rely on iterative prompting loops where the LLM self-determines status and next steps. However:  

LLMs often misjudge progress or get stuck in loops.  
Their internal reasoning is opaque.  
Costs accumulate because prompts grow long with history.  
Thus, the authors propose **explicit state grounding to regulate transitions** and systematically manage prompts and external tool calls.  

## Implementation & Case Study: SQL Task

Using the InterCode SQL benchmark, the authors abstract a workflow from the ReAct trajectory:  

Init â†’ Observe â†’ Solve â†’ Verify â†’ End
            â†˜
            Error

Init: always executes â€œSHOW TABLESâ€.  
Observe: runs â€œDESCâ€ to explore table schemas.  
Solve: constructs SQL query via SELECT.  
Verify: checks correctness (self-evaluation).  
Error: recovers from execution failures (e.g., wrong table name).  
Transitions depend on tool feedbackâ€”e.g., failed SELECT â†’ Error; successful DESC â†’ Solve.  

Result: 
â†’ Major cost reduction due to shorter prompts (~400 vs 2000 tokens).  
â†’ Explicit Error and Verify states are essential for performance robustness. 


## Findings

**Strengths**:

Dramatic cost reduction (3â€“5Ã— less API use).  
Improved interpretability: each step traceable.  
Ease of combination: compatible with Reflexion, ToT, etc.  
Generalizable: shown effective in coding, command-line, and embodied tasks.  

**Limitations:**. 
Requires manual design of states and transitions.  
Not fully autonomous â€” needs domain knowledge.  
May underperform in tasks with unpredictable flows.  

**Future directions:**   
Automating state discovery and transition definition via LLMs.  
Adaptive StateFlow (auto-add/remove states via performance feedback).  
Integration with active learning or policy gradient loops for self-improving workflows.  

---

## References
â€¢ StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows. arXiv: [https://arxiv.org/abs/2403.11322]

`
    },
    'ReAct': {
        metadata: {
            title: 'ReAct: Synergizing Reasoning and Acting In Language Models',
            date: '2025-10-04',
            tags: ["LLM","AI Research"],
            summary: ''
        },
        content: `## Motivation

**Summary** 

â€¢ ReAct stands for Reasoning + Acting. Traditional prompting often makes an LLM either: reason (CoT, logic, planning), or act (produce an action such as tool calling or retrieving information).  
â€¢ ReAct combines the two in a single framework: the model can reason, then act, then reason again, similar to how humans solve tasks interactively. 

**Problem**

â€¢ Chain-of-thought reasoning is a static black box, in that the model uses
its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process.

**Why This Matters**

â€¢ There have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic benefits compared to reasoning or acting alone.  
â€¢ ReAct offers a way to combine both modes, making them more useful for real-world tasks (search + reasoning, planning + execution, or interactive decision making).  
â€¢ Itâ€™s a step toward more agent-like LLMs, which can reason, act, observe results, and adapt.  

![ReAct1](images/ReAct1.png)

**TDLR;** 

ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an **interleaved manner**, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).  

---

## Methods

**ReAct Formulation:**

â€¢ Consider a standard agent loop: at time t, the agent sees observation oâ‚œ and chooses action aâ‚œ per policy Ï€(aâ‚œ|câ‚œ), where câ‚œ is the entire trajectory so far (all past observations and actions plus the current observation).  
â€¢ Expand the action space from A (external actions) to Ã‚ = A âˆª L, where L is the space of language thoughts (internal â€œnotes to selfâ€). L does not alter external environment but update the context.  
â€¢ By allowing "think stepsâ€: The agent can decompose goals, plan, recap what it just saw, inject background knowledge and update its planâ€”all in its own wordsâ€”before choosing the next real action. This reduces blind moves and tool-use hallucinations.


**What this looks like in practice**
Task: â€œWho wrote The Selfish Gene and what year was it published?â€  
Observation oÂ¹: The question text.  
Thought Ã¢Â¹: â€œPlan: search for the book, then extract author and year.â€  
Action aÂ¹: search("The Selfish Gene author"). 
Observation oÂ²: Search results snippet shows â€œRichard Dawkinsâ€.   
Thought Ã¢Â²: â€œAuthor found; now get publication year.â€  
Action aÂ²: open("The Selfish Gene Wikipedia").  
Observation oÂ³: Page shows â€œPublished in 1976.â€  
Thought Ã¢Â³: â€œWe have author and year; compose answer.â€  
Action aÂ³ (final): â€œRichard Dawkins, 1976.â€  
Notice how thoughts donâ€™t change the environment; they just clarify and steer the next step.  

The paper focuses on a frozen LLM (PaLM-540B) with few-shot **in-context trajectories** that interleave Thought/Action/Observation.  

Two patterns the paper highlights:   
â€¢ Reasoning-heavy tasks (e.g., multi-hop QA, fact-checking): Alternate Thought â†’ Action â†’ Observation, repeatedly.  
â€¢ Decision-heavy tasks (e.g., games, web navigation): Thoughts appear only when useful (sparsely), and the model decides when to insert them.  

---

## Experiment

Setup & action space. The model interacts with a minimal Wikipedia API:
(1) search[entity] returns first 5 sentences or suggestions; (2) lookup[string] returns the next sentence containing a string (like Ctrl+F); (3) finish[answer]. The API forces explicit, step-wise retrieval and reasoning rather than relying on a powerful retriever. 

Few-shot prompts. 6 (HotpotQA) and 3 (FEVER) ReAct trajectories with dense thoughts were hand-crafted; thoughts include goal decomposition, extracting facts, commonsense or arithmetic checks, search reformulation, and synthesis.

![ReAct1](images/ReAct2.png)
Hybrids win: ReActâ†’CoT-SC (HotpotQA best) and CoT-SCâ†’ReAct (FEVER best).  

--- 

**Key Results:**
Why hybridization helps (error analysis). Manual labeling of successes/failures shows:

CoT hallucinates facts more (false positives and major failure mode), ReAct is more grounded but suffers more reasoning errors (e.g., loops or weak search) due to its structural constraintsâ€”hence the benefit of combining internal CoT with externally grounded ReAct. 

Switching heuristics. If ReAct canâ€™t answer within a small step budget, fall back to CoT-SC; if CoT-SC has weak consensus, fall back to ReAct. This reaches CoT-SC-level performance with far fewer samples (3â€“5 vs 21).

---

## References
â€¢ REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS. arXiv: [https://arxiv.org/abs/2210.03629]
`
    },
    'cot-without-prompting': {
        metadata: {
            title: 'CoT Reasoning Without Prompting',
            date: '2025-10-03',
            tags: ["LLM","Reasoning","Decoding","AI Research"],
            summary: 'Paper explores how chain-of-thought reasoning can emerge without explicit prompting, by modifying decoding strategies instead of altering the training or input prompts.'
        },
        content: `## Motivation

**Problem:**   
Current Chain-of-Thought (CoT) reasoning in LLMs is typically elicited via prompt engineering (few-shot, zero-shot, instruction tuning).

**Issues with prompting:**  
â€¢ Requires manual, task-specific engineering  
â€¢ Hard to separate model's intrinsic reasoning ability, or if they are just mimicking human-provided reasoning formats  
â€¢ Instruction-tuning with CoT data improves performance but is expensive

**Key Question:** Can LLMs reason without prompting?

**Traditional decoding (greedy search):**  
â€¢ Outputs short, direct answers  
â€¢ When decoding via top-1 token selection, model often jumps straight to an answer without showing reasoning steps

**CoT decoding:**  
â€¢ Explores **top-k alternative tokens** during generation  
â€¢ Many reasoning paths are inherent in the model, just not revealed in greedy decoding

---

## Methods

**CoT Decoding Process:**  

â€¢ **Step 1:** At the first decoding step, branch into top-k tokens  
â€¢ **Step 2:** Continue greedy decoding along each branch  
â€¢ **Step 3:** Extract CoT paths by ranking with a confidence metric:  
   Compute Î” = average margin between top-1 and top-2 probabilities for answer tokens  
   Higher Î” â†’ model more confident â†’ often corresponds to a valid CoT path  
â€¢ **Step 4:** Pick the decoding path with highest Î” (or aggregate across paths)  
    This reliably identifies reasoning-consistent outputs  
    **Similar to self-consistency, but without the prompts**

![CoT Decoding Process](images/cot-decoding-diagram.png)

---

## Experiment

**Models Tested:**
â€¢ PaLM-2 (X-Small â†’ Large)
â€¢ Mistral-7B
â€¢ Gemma-7B
â€¢ Pre-trained and instruction-tuned variants

**Tasks/Datasets:**
â€¢ **Math:** GSM8K, MultiArith
â€¢ **Commonsense:** Year Parity
â€¢ **Symbolic reasoning:** Coin Flips, Web of Lies, Multi-step Arithmetic (Big-Bench-Hard)
â€¢ **Synthetic tasks:** Sports Understanding, Object Counting

![CoT Result](images/cot-greedy.png)

---

## Findings

**Key Results:**
â€¢ CoT reasoning can be elicited **without explicit prompts**
â€¢ LLMs already learn reasoning patterns during pretraining
â€¢ Using greedy decoding underestimates a model's true reasoning ability

![CoT Result2](images/cot-result.png)

**Future Directions:**  
â€¢ **Adaptive Branching:** Decide dynamically when and where to branch during decoding. 
â€¢ **Training integration:** Use discovered CoT paths as training signals for fine-tuning. 

**Comparative Analysis:**  
â€¢ **Greedy decoding:** Fast but hides reasoning.   
â€¢ **Top-k, top-p, beam search:** Increase diversity but not reasoning accuracy.  
â€¢ **Self-consistency:** Needs CoT prompts, aggregates across multiple outputs.  
â€¢ **CoT-decoding:** Purely decoding-based, unsupervised, more faithful measure of intrinsic reasoning ability.  
â€¢ **Hybrid (CoT-decoding + prompting):** Best of both worlds; achieves state-of-the-art reasoning accuracy.  

**Note:**  
â€¢ **CoT-SC** aggregates prompt-elicited reasoning trajectories. 
â€¢ **CoT-Decoding** uncovers and aggregates intrinsic reasoning trajectories that emerge naturally in the decoding process without any prompt or instruction-tuning. 

---

## References
â€¢ *Chain-of-Thought Reasoning Without Prompting*. arXiv: [https://arxiv.org/abs/2402.10200](https://arxiv.org/abs/2402.10200)

`
    }
};

// List of note IDs in order
const NOTE_IDS = ["Voyager","CER","reflexion","stateflow","ReAct","cot-without-prompting"];


// Load embedded notes instead of fetching
function loadAllNotesEmbedded() {
    const notesContainer = document.getElementById('notes-container');
    if (!notesContainer) return;

    notesContainer.innerHTML = '';

    NOTE_IDS.forEach((noteId, index) => {
        const note = EMBEDDED_NOTES[noteId];
        if (note) {
            const noteCard = createNoteCardEmbedded(noteId, note.metadata);
            noteCard.style.animationDelay = `${index * 0.1}s`;
            notesContainer.appendChild(noteCard);
        }
    });
}

function createNoteCardEmbedded(noteId, metadata) {
    // Create clickable wrapper link
    const link = document.createElement('a');
    link.href = `study-detail.html?id=${noteId}`;
    link.className = 'note-card-link block';
    link.setAttribute('aria-label', `Read note: ${metadata.title || 'Untitled'}`);

    const article = document.createElement('article');
    article.className = 'note-card';

    const headerDiv = document.createElement('div');
    headerDiv.className = 'flex items-start justify-between mb-3';

    const title = document.createElement('h3');
    title.className = 'text-2xl font-semibold text-gray-900';
    title.textContent = metadata.title || 'Untitled';

    const date = document.createElement('span');
    date.className = 'text-sm text-gray-500 whitespace-nowrap ml-4';
    date.textContent = formatDateEmbedded(metadata.date);

    headerDiv.appendChild(title);
    headerDiv.appendChild(date);

    const summary = document.createElement('p');
    summary.className = 'text-gray-700 leading-relaxed mb-4';
    summary.textContent = metadata.summary || '';

    const footerDiv = document.createElement('div');
    footerDiv.className = 'flex items-center justify-between';

    const tagsDiv = document.createElement('div');
    tagsDiv.className = 'flex flex-wrap gap-2';

    if (metadata.tags && Array.isArray(metadata.tags)) {
        metadata.tags.forEach(tag => {
            const tagSpan = document.createElement('span');
            tagSpan.className = 'tag-small';
            tagSpan.textContent = tag;
            tagsDiv.appendChild(tagSpan);
        });
    }

    const readMoreText = document.createElement('span');
    readMoreText.className = 'text-gray-900 font-medium text-sm inline-flex items-center';
    readMoreText.textContent = 'Read More â†’';

    footerDiv.appendChild(tagsDiv);
    footerDiv.appendChild(readMoreText);

    article.appendChild(headerDiv);
    article.appendChild(summary);
    article.appendChild(footerDiv);

    link.appendChild(article);

    return link;
}

function loadNoteDetailEmbedded(noteId) {
    const note = EMBEDDED_NOTES[noteId];

    if (!note) {
        document.getElementById('note-content').innerHTML = '<p class="text-red-600">Note not found.</p>';
        return;
    }

    const { metadata, content } = note;

    document.getElementById('page-title').textContent = `${metadata.title || 'Study Note'} - Enoch Lee`;
    document.getElementById('note-title').textContent = metadata.title || 'Untitled';
    document.getElementById('note-date').textContent = formatDateEmbedded(metadata.date);

    const tagsContainer = document.getElementById('note-tags');
    tagsContainer.innerHTML = '';
    if (metadata.tags && Array.isArray(metadata.tags)) {
        metadata.tags.forEach(tag => {
            const tagSpan = document.createElement('span');
            tagSpan.className = 'tag-small';
            tagSpan.textContent = tag;
            tagsContainer.appendChild(tagSpan);
        });
    }

    const contentDiv = document.getElementById('note-content');
    contentDiv.innerHTML = marked.parse(content);
}

function formatDateEmbedded(dateString) {
    if (!dateString) return '';
    const date = new Date(dateString);
    const options = { year: 'numeric', month: 'short', day: 'numeric' };
    return date.toLocaleDateString('en-US', options);
}
